# Tiny GPT

Exploring the internals of large language models. Using a tiny shakespeare dataset, we train a small GPT model from scratch.

- Tokenization is done be encoding every character into a unique number (use vocab index).
